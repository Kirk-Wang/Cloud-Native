This is a part of a discussion we need to have about proper

liveness checking. This is an excellent example

of what seems to be a good liveness check.

But, it turns out to be a bad liveness check.

Because not only is the liveness check

dependent upon whether there's load.

In other words, the load on the container is causing

the liveness check to fail when there wasn't really

anything wrong with the container, it was just slowing down

due to load.

It was still responding. It was just taking a while to

respond. Because our liveness probe was so

short, at that one default second, it would start

to fail, which led to cascading failures.

Because in this case, even if we had 10 rng's, it

would have depended, while that first one was restarting,

it would've depended more on the other nine.

They would have started failing, as well, because they were

receiving more connections since there's less of them.

Then, that would sort of end up in a cascading failure

scenario. This is one of several common scenarios

where we need to think about our liveness checks,

and our readiness checks, and how they

would be handled in a situation where there's excess

load. So many of our applications, especially on the

Internet, you know, can sometimes receive exponential

amounts of traffic. Whether that's a good person, that's

just using your system a lot more than normal, or a bad

person who's trying to denial of service you.

And, denial of services tend to be pretty effective

unless the system is really designed in a robust way.

Because denial of service is all about sending a bunch of

connections to you that aren't useful or aren't for

the good of the system.

In this case, our liveness check was working against

us. It was shutting things down that were just a little bit

slower than normal, but still responsive.

In this case, we would essentially look at this liveness

check and say, OK, this really shouldn't be a liveness

check. This should probably be something more like a

readiness check, because what we're saying is the container

was still responsive. It was just slow because of too much

load. That's a good example of when readiness is

appropriate, because we're saying, let's maybe don't

send more connections to that container so that it can

catch up. So, we'll take new connections and send them

elsewhere.

That way, we don't keep adding more load to that

overly worked container.

Now, this still isn't a perfect scenario because

we're saying that the other containers are going to get

more of the load because we're taking the load off of the

one that's slow. So, that's not really solving this

problem. In the real world, you would probably want to

start looking at something called Horizontal Pod

Autoscaler, which is an optional feature inside

of Kubernetes that will allow us to spin up more

pods based on metrics, including

a metric that we could check on the number of incoming

connections. So, a simple example here would be having

an autoscaler that is looking at this particular

deployment of rng.

If it sees a number of connections, let's say

eight average connections per pod, then it would know to

spin up more pods.

We haven't gotten to that point yet.

For now, let's just think about this as maybe what we

should do here is have a readiness check

that is a shorter timeout, maybe just a couple of seconds.

We'll fail more often and allow

the container to catch up with the work it's been given.

Or, in a liveness check,

we need to give it a much longer timeout, possibly

even five seconds, maybe even ten seconds, just so

that reasonable amounts of load can be applied

to the container, and it has time to respond.

We just want to make sure we don't have a system

where our checks are the reason things are failing.