This leads us to a really great feature

in Kubernetes for the more advanced situations where

you need to get information about the

pod, or container itself, into a container

for the application to see it.

That's known as the Downward API and it's a feature that's

a little bit like introspection. It allows a container to

see information about itself that's not normally

inside the container.

We can do that by sending information in

at startup time, either through environment variables

so that the app just sees different environment variables

and that information is different parts of the container

and pod spec.

Or, we can put in a volume and have

each one of these Downward API key/values.

We can put them in a volume as a separate file each.

Now, that second option is pretty rare.

So, that's why we're saying that we're not going to do it

right now. I'd say the most popular option, for

sure, is to send them in as environment variables.

That tends to flow well with your custom applications

because you can build in these environment variables and

hardcode, them like we do here, to start

with. Then, when you need more flexible things like

information from the pod and container spec, that

will show in a second, you can then programmatically put

that in so that it updates every time the pod relaunches.

Here's an example of that.

This is a standard name and value

from an environment variable.

The name is fine, right. We just make up whatever name we

want. Doesn't really matter. It's whatever we want the

container to have as an environment variable.

But, instead of giving it a hardcoded value,

we're telling it to pull a value from something else.

In this way, we're able to say

we're going to specify a field reference and then that path

of that field is our pod

spec. In this example, what we're saying is

from my pod spec, please fill this

value with the metadata.namespace.

If you've looked at yaml.notation, you can see

that it's similar to JSON notation where you can drill

down in a spec through the dots.

A good example in this case, why you would use that,

is you maybe want to launch the same application

over and over in different namespaces, and you want

the application itself to know which namespace it's in.

So, you can feed that in as an environment variable.

Then inside your application, it just has to know to look

for the MY_POD_NAMESPACE.

An interesting example of something in DNS we haven't

talked about yet, but we'll talk more later in the

networking, is that every namespace

technically can be reached by other namespaces

using the fully qualified domain name.

We'll detail more about that later, and what that

means really, and how you can avoid letting networks

access each other, and all that stuff.

This is all related to the CNI, the container networking

interface. We have just touched on that a little bit.

But, once you add your plugins in for the networking, you

can decide whether you want different namespaces to access

each other. If they do, they would use a fully

qualified domain name here with the namespace

somewhere in that path, just like you see

it here. Then, the name of the service.

You don't necessarily need to do that that often.

But, it's an interesting feature on how to use this.

You could also change that instead of just pulling out the

namespace, you could pull out lots of other values of the

pod and container spec.

In this case, we're getting the status.podIP.

Remember, multiple containers in the same pod

all share the same IP.

So, the IP of the container on the

Kubernetes network is the same as the rest of the pod's

containers, and it's going to be in this status.podIP

because it's created at runtime.

The nice thing is, is we can get that into the container by

adding it into an environment variable that we set.

A more likely scenario is when you're

dealing with apps that have resource limits,

and you want the app to be able to know what those resource

limits are. Not every application on Linux knows

how to deal with cgroup limits, which

are resource limitations around memory and CPU, networking

and I/O. When we do that

with Kubernetes, the application may need to

be manually configured to link,

basically identify, what its resource limits

are. So, in the case of Java with the JVM,

for the longest time, one of the challenges with it

is that it doesn't know about cgroups.

It doesn't recognize cgroup limits if you set them

on it's container. All it did in the past was

look at the maximum memory of the

server, and then it would try to use all of that.

Obviously, that's bad when you got lots of things running

on the same system.

In recent versions of Java, this is now fixed because Java

can now see the limits in cgroups.

But before that, we would have to configure

the JVM with the Xmx option,

and you would then have to hard code it,

right. You'd have to go into your YAML and say, well, this

has to match whatever my resource constraints are.

The resource constraints in Kubernetes are very similar to

the same ones you would use in Docker where you can set up

CPU and memory, and other things.

In this case, what we're able to do, then, is

I'm able to take the resource limits that I

set in the spec, send them into the container

at startup, set it to an environment variable of

MY_MEM_LIMIT, or whatever else I want to set it.

Then, on the start command for my JVM,

I would put in the Xmx and then

put that environment variable in there.

What this means is, is that this container would start

appropriately with the right settings, and the only thing

I have to change is my resource limits in Kubernetes spec.

There's lots more documentation on the Downward API,

including the file-based method with volumes

on the Kubernetes site.

Go check that out with the links in this video for the

resources, in case you want to configure this kind of

stuff in your pods. But, it's important you know that

that option is there because it's a pretty neat one.

