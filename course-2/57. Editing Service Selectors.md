All right. We're back and we're going to try editing this

service again to change the selector.

This time, we're going to put in quotes around the yes

value to make sure it's not tricked into

thinking it's a Boolean. Which, we know better, but the

computer is smarter than us.

Let's go in and edit that same file, and

let's go down to the selector.

Remember, we've got to go under spec first, then find the

selector. Then we're going to add that enabled equals yes,

in quotes. enabled="yes"

and you can use single or double quotes here.

It doesn't matter. All right, let's save it.

Exit and yes. No errors.

If we did it right and we go back to our browser, we

still shouldn't see any changes in the UI

because we haven't told the service

anything that's going to change the result.

It still is looking for enabled yes.

First, if you remember, we added the enabled yes label

to the pods. So, it still has that complex selector

of looking for app and enabled,

those with the right values.

Since it sees both of those in both pods, we're good to go.

We haven't changed anything yet.

Now, we come to the point where we can control

which pods it's sending traffic to.

Our goal here is to get rid of the deployment because we're

now moving to this fancy new DaemonSet.

There's several ways we can identify, and there's several

ways we could change it. We could go in and use the

label command to find the exact pod.

We just basically do a get pods, find the exact pod, or

look at describing the deployment, and then

drill down to finding the right pod.

There's several ways we can get to that actual name.

We can use a label selector to look for the

only pod of that deployment that has this

particular hash.

There's other ways to do it.

This is our plan. We're going to use another environment

variable to make this all command line driven

so we're not doing any cutting and pasting.

We're simply typing commands.

We're going to take and use a get pod, with its own

selector, that looks for those two

labels, and takes the name of it, puts it

in the pod variable, and then we're going to use a logs

command in one terminal.

So, you're going to need two terminals for this because

we're going to watch the connections stop to this pod

on a simple label change.

This will prove to us that this is really working.

On one screen, we're going to be watching the logs of that

pod. Then in the other screen,

we're going to take away the label from that pod

and watch the traffic stop.

All right. I'm going to put the logs on the right.

Again, we're going to type pod=

and then we're going to have a sub command, kubectl

get pod, with the selector of app=rng,

and also the pod template

hash. I'm not putting a value there.

You notice that if I don't put the value, it just looks

for the existence of it, because all we need is for it to

exist because then we know it's from a deployment,

technically from a ReplicaSet, which is from the

deployment. Then we're going to ask it to output just the

name. That's going to save that name

in the pod environment variable.

We're going to use that pod environment variable in a logs

command. Again, we could've just done a get pod,

found the right name for the right pod, and then

just copy and pasted it.

But, you know, we're doing shell stuff.

So, this is "funner." tail 1.

We're going to start with the latest log entry

and not give us all the logs from the last hours

worth of work. Then follow

the pods so that we can see it live.

Then, we can see the traffic that's going pretty quickly

multiple times a second.

On the left side, we're going to kubectl label

pod because we're only going to do this against a pod.

Then the selector is going to be that same selector,

app=rng,pod-template-hash.

Then, we're going to ask it to remove a label.

The way you do that in kubectl is the type enabled

with a dash at the end. You've got to keep the dash right

next to enabled. All one word.

That's asking it to delete the label enabled regardless

of the value. All right.

Did you notice on the right side that our logs

have frozen?

They're not really frozen.

It's just no longer receiving traffic because it was almost

an instant update.

The service instantly realized

that it only has, now, one pod that meets its

two-label selector.

The pod is still there.

The pod, we are still connected to.

We're still seeing the logs of it, but it's no longer

receiving traffic.

Now, where does that leave us?

We're no longer in need of the deployment.

We could technically go delete the deployment.

Now, we're left with the DaemonSet.

If you're like me and you only have the one node for

learning, then we only have the one DaemonSet rng.

If we start adding more nodes to our cluster,

we would start to see the rng scale up.

If you go look at the miner, I will

have slowed down a little bit because I've lost one of my

two rng containers.

So, I'm not going to mine as many coins.

What we're saying here is that definitely adding more

rng's would help, but the imaginary game

we're playing is that it needs more randomness,

and that we have to have more nodes for that.

Obviously, that part's fiction.

It was just part of the story to give us an excuse for

creating DaemonSets.

If we really wanted to ramp this up right now, we could

technically just get rid of the DaemonSet, go back to the

deployment, and ramp it up to 10 rng's, or something, and

we could probably get a lot more coins.

But, an important point that we don't want to forget is

that we haven't changed the DaemonSet.

Right now, the state we're in, is that the service

is looking for two labels in order to send traffic to the

pod. But, the DaemonSet doesn't

know that it's supposed to add two labels to each

new pod it creates.

Which means if you went, right now, and added a second node

to your cluster, we would get a new pod,

but it wouldn't match the selectors of

the service. The service wouldn't see it because it's only

got the original label.

We would then have to go edit the spec

for that rng DaemonSet.

In fact, we can just go take a quick look at that.

If you scroll down in that file.

Whoops. Yeah. Scroll down past the metadata.

Then the selector that we would not

change here because this is not the service, the service is

where the selector changes.

Here we need to tell it, the template for making any new

pods needs to have both labels in it.

This is where we would add it.

