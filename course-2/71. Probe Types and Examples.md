There's multiple benefits of looking into

using Healthchecks.

I can't stress enough that you totally should be using

them. Because if you don't, you lose a lot of the

intelligence around, not just deploying or checking

if your apps are OK, but the rolling updates,

and a lot of the other stuff in Kubernetes become so

much smarter when healthchecks are in all your containers.

If you don't have these, then a rolling update only knows

that your container is started.

It doesn't know the state of your app, if it's ready for

connections, does it need to wait longer for startup?

It doesn't know any of that stuff.

So, that's why these are so important in a world where

you're going to be changing your apps and deploying new

versions regularly.

You need to have this kind of stuff.

A third-party monitoring solution never really provided

that level of automation.

So, before we had this kind of feature in container

orchestration, we often had to write advance

scripts to manage load balancers while apps were starting,

and all this other stuff.

Well, now with healthchecks, we don't need that.

It's all built in out-of-the-box, as long as you're

enabling them. For the three types of handlers,

they're not too complex. The HTTP one is expecting

an HTTP response code of either 200

or anywhere between that and 399.

It's typically 200, and that's normally how you would have

a response. But, 400 and higher is some form of

error. If you ever get that in a healthcheck, then

it will consider it unhealthy.

Next, with the TCP connection, it's doing a

simple TCP open.

If it can connect to that port, then it

works. At least it thinks it does.

The last one is similar to Docker healthchecks, which we

don't use technically in Kubernetes. The Docker

healthchecks are just for Docker, Compose and

Swarm. But in Kubernetes, these can

run a Docker exec in the container and run a

script, or any commands, you want on the local system where

that container is running. The one little caveat here

is that this tends to be the most expensive type of check.

So, you want to be careful with these.

When I say expensive, what I mean is it

will end up using more resources because it has to launch

another shell and run some form of app, or script,

that you've written. That's a lot more work than the

kubelet process simply checking an HTTP

port for a response code, or checking a TCP open

connection. When you do execs, just realize that if

you start doing them really frequently, like once a

second or once every five seconds, and you have hundreds

of containers, or even just a dozen containers in

one of your deployments, then it's doing that on every

single container, every five seconds.

And, you might actually generate load where you would

see, once you've enabled these, you would actually see your

CPU, or your I/O, go up on

your servers because you're putting so much work into it.

One of the keys of all three of these is to keep them as

simple as you can, while also improving

the intelligence of them, so that they know really whether

your app is healthy. But,

also be careful of the ones like exec that tend to be more

expensive in terms of resources and CPU load.

Now, out-of-the-box, we have some defaults.

Here is a list of some of the settings you're going to be

using here in your YAML, and the defaults that come

out-of-the-box that you can, of course, change.

The four that you need to care about the most are the four

you see listed in order here. The periodSeconds, the

timeoutSeconds, the successThreshold and the

failureThreshold.

Those four are what you're going to be looking at for

almost every one of your checks.

You're going to want to customize those.

Usually, the success and failures are default,

but depending on how fast you want your

app to move in and out of a healthy state, you might want

to change those. Any time the success happens,

meaning that a healthcheck has been successful

just once, then it considers it healthy again.

But it takes, by default, three failed healthchecks

before it will do something about it.

That's nice because you don't always want an app to

immediately be reckoned with by Kubernetes

when it fails one healthcheck.

Let's look at

a couple of examples of what these probes would look like. This

one would be for the rng, the random number generator.

You can see here, for the YAML of

that pod, we're seeing, in the container spec, the

liveness probe is using an HTTP handler, and

we're giving it an HTTP path.

Then, we're telling it what port to try to connect on.

Then, we're giving it a 10 second initial

delay. So, it'll wait 10 seconds after

the container has started up before it tries

this connection. This would probably be in the 1.16

and newer versions. You would probably change this to these

startup type of healthcheck, and you wouldn't need this

anymore because this

was really what we were trying to do in earlier versions to

prevent the check from happening too early when the

container's still starting.

So, this is something that you might want to move to

startups. The periodSeconds means it's going to do it every

second, which is pretty aggressive.

In our test app, this will be fine, but

we're going to test it out in a second, and

we'll see what happens when you do these too often. For our

redis server,

in this case, this is a really good example of a database

system that you can't just connect

to with HTTP.

And, it also wouldn't be very intelligent if

we just simply connected to a TCP port.

That's not very intelligent. It can't really see anything

in the application. But with redis, and

a lot of other database systems, there is a command that

comes as a utility with the app that

you can use here. In this case, we're doing a liveness

probe. We're doing the exec handler.

This command is running inside

the container using this redis CLI utility

that comes with that container image, because it has to be

in the image. Then, it's going to run in a

shell, redis-cli ping.

That utility is designed to exit 0,

which means to not return an error when it quits.

And if it exits 0, then Kubernetes thinks it's

healthy. But if it exits with some sort of error code, like

exit 1 or exit 127, or any other error

code, it will assume that it's unhealthy.