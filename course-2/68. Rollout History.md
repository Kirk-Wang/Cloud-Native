So far, we've been typing these commands with version

numbers that are easy for us humans to remember.

So v0.1 and 0.2.

Those are easy for us to remember which one is newer

than the older and all that stuff.

The order of them. That's a little bit like semantic

versioning, what we're using here.

But that's often not what you're going to be using in

production, especially once you start to automate

and build out your infrastructure, and your deployments

through your CI solutions or GitOps or

something like that. You're going to be automating these

rollouts, and the numbers in the tags of your images

aren't going to be easily memorable.

They're often going to end up being git commits.

So, it's like the short version of the git commit, or maybe

something out of your continuous integration solution

that automatically generates a unique number, something

like that. Maybe it's even date based, but

all of these things are not going to be easy for you to

remember versions. What can we do in Kubernetes

to deal with older versions and then sort of investigate

the ReplicaSets, the deployments that we're rolling out?

Well, we have some ways to do that.

The first is that easy command, kubectl rollout history.

That will give us a history of the deployment itself.

Let's try that.

history deployment worker.

We're giving it the resource that we can see the rollout

of. And in this case, I have one, five and six.

Now, depending on how you were playing around in previous

lectures, you might have different numbers.

You might have three, four or five just depending on

exactly the commands you type, in the order you type them

in.

The reason this isn't consistent is because

the deployment is one type of resource

and the ReplicaSet is a different type of resource.

There's a history difference to the two of them.

You can change a deployment and it won't change the

ReplicaSet, and vice versa.

So, the ReplicaSets themselves have a revision

history that we don't see here, and that's

a part of the trick. It's almost like a nested history,

depending on whether you undo or not.

If we just change the deployment and kept doing that,

we would probably have all these consecutive numbers.

But, because we started doing undo's, there's technically

a history in the ReplicaSets.

So, how do we see that?

That's some data we can find, and it's stored in something

called annotations.

Now we'll talk more about annotations later

where they're one of the two main types of metadata.

We've talked about labels. That's the first type.

Annotations are the other type.

We'll talk about that a little bit more. You'll also notice

on the screen, by the way, the change-cause.

That's something that we're going to talk about later.

That's also stored in annotations, if you want to put it

there. Back to these revisions

of the ReplicaSet. We can get at that a little

bit by seeing what our annotations are

on our deployment.

So, we can run this command here where the describe command

puts out all of the ReplicaSet information where

these annotations are stored.

Then we're going to grep the -A3 Means to

say, give me the next three lines after

the matching word, and the word is annotations.

We'll get a nice, concise layout of all the

different deployment revisions.

Let's try that now.

kubectl describe replicasets.

Then we're using a selector here, app workers only show

me the worker ReplicaSets.

Then we're going to grep it. If you don't have grep, you

can use the shpod that we had

before that gets you into the cluster.

And grep -A3.

So, for the three lines after the matching line,

annotation.

Now, what we should see here, in my case, because I had

three of those deployment revisions,

I can see all three of those here, and this is where

it gets that revision from.

All right. So the ones I saw on that screen were 1, 5 and

6. You can see that I have those here.

Again, yours might be slightly different numbers depending

on the order of the commands and how many undo's you did.

Then you can see here that for each one of these

revisions, there is their own revisions

as a nested history.

If you add all these up, we technically have all six

here, but you can see the 1 is up at the top, the original

revision. Then 2 is hidden down

here in the ReplicaSet history.

Then 3 is down in this other ReplicaSet history.

So, it's like a nested history branch thing we could

probably draw out, and it gets a little complicated.

But the point here is that when you do an undo,

that is treated in the revision history a little bit

different.

There, unfortunately, isn't a command for us to dig into

those revision histories and lay it all out.

We could probably script this because all of this is stored

in the annotation metadata and we could

extract that out. But my experience is, is that you

typically aren't doing a bunch of nested

undo's.

You're not undoing and then undoing again, like we just

did. You're not typically hopping back to

a bunch of different versions all the time.

So, usually the undo the most common use

of something like an undo, is when you're rolling something

out and you realize that this new version is not going to

work right. And you know that the old version did.

So typically, in the real world production, I just see

people doing a single undo manually.

In some cases, if the rollout fails healthchecks, which

we'll talk about later, the healthchecks will do an

automatic rollback.

So, we don't really have to do that necessarily, as a

human, if we have really good healthchecks in our app.

But there's always, of course, the chance that something

goes wrong that you can't see in your monitoring solutions,

or your healthchecks, and then you have to manually do that

undo. So, in this case, we can do

a manual rollback, and we're going to rollout the

revision 1, which is our original v0.1

version that we had.

All right. kubectl rollout undo

deployment worker

--to-revision=1.

All right. So, technically it knows that it's doing an

undo, and it's going to roll us back to that saved

initial version of the rollout.

Now, let's go check our WebUI to see if we got

version 1 back to a healthy state.

You should be now back at the original

v1 10 coins per second.

So, we've technically had a failed three.

The two successfully went out, but the version 0.2

didn't work, so we needed a rollback to version 1.

There's a couple of more ways that we can roll back

or change these deployments.

I'll talk about that in the next lecture.

