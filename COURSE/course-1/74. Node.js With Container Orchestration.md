Once you've started using Node.js in production, you're

eventually, if not already, going to be using

orchestration.

Orchestration is something that lets us use many

different nodes, one or more, to act

like one, single Docker engine.

You might be using Swarm, ECS, Kubernetes,

Mesos, Nomad.

There's a lot of options.

The two most popular are the distributions of Kubernetes,

whether that's OpenShift, or Amazon's option,

or DigitalOcean's option, or Azure.

Really, all those Kubernetes options use standard

Kubernetes deployment files and configurations.

We're going to be able to look at that in a couple of

lectures, as well as the next lecture where we'll talk

about Swarm specifics for Docker Swarm.

Right now, I just want to get you refocused on the few

things that really matter when you're dealing with Node

in orchestration. When I say orchestration, what I really

mean is using more than one container,

from the same image, whether that's on the same host

or multiple hosts.

Because then you usually have to deal with things like load

balancers, or concurrency, or storing

state, depending on what type of app you're running in your

Node container. The big, three areas that you need

to focus on, which we've covered in this course, are

the startup and making sure that you have healthchecks,

or readiness checks, to ensure that your container

doesn't go live for whatever clients it needs,

before it's ready to be live.

Second, is dealing with state.

That's common even before containers when

we had multiple server setups.

You had to worry about state of your apps.

Usually you store the web state, or your session

state, or whatever the state of the container, you usually

store that in a database.

In an example in a minute, I'll talk about sockets I/O

and how even that requires state if you're using a multi

container setup. So, sockets I/O, we'll need to use

Redis backend or something to store your web socket

sessions. Then most importantly, is when

you shut down the containers, or replace them in most

cases. We're not often getting rid of containers.

We're usually replacing them with a new version or

something else. When you do that, you're going

to need to make sure that you have proper shutdown so that

your healthchecks, your liveness, your readiness.

All these different types of checks, whether it's Swarm, or

Kubernetes, or just regular Docker.

All those things properly shut down.

You properly close database connections and tell clients

that they need to reestablish their connection so that the

load balancer can move them to the proper node.

There's all sorts of stuff to worry about there.

These are the three areas that you need to work on and test

thoroughly to make sure your app works right before you go

production. You've seen us use the voting app before

in this course. I wanted to give you one, last little

glimpse of what result the result app, if you remember that

web app, it was using angular sockets

I/O, Express, sort of standard stuff that you

might see in a Node web app setup.

That result app, for the voting example app, I

have updated it inside this directory.

So, you go check this one out, and I basically have put a

few of the production concerns in there.

I've created a healthcheck URL, a

liveness and readiness check URL for Kubernetes.

I've added Stoppable, so it will track connections and

properly shut them down.

I have a whole shutdown process in there that will

disconnect clients through Stoppable and then shut down the

database connection, and then properly tell the

healthchecks to stop allowing connections.

And other things like using Redis as our storage

backend for sockets I/O.

And on and on and on. There's stuff in there that you

definitely should check out. I would not say that this is

production quality code.

There is very little error checking, but I've at least

added some of the concerns, like the termination

signals received from Docker and dealing with those.

Let's go check it out.

Jump in the sample result orchestration

directory and then open up the server.js file.

That's where I'm going to be looking at this.

From the top, I'm just going to scroll through this

really quickly to show you, in case you're not looking at

it yourself. You're maybe just watching this on the go.

Some of the things that I've added in here to a standard

Node app that maybe was precontainer like this one.

This is a Legacy app.

This was actually built on Node 6, so I've updated

the Dockerfile to 10, but that's not really important here.

I could have changed some of these things to be more modern

JavaScript and ES6 ready, but that's not

the case. I just really wanted to plug in here some of the

features that were probably necessary in a production

cluster.

You'll see that I have added Stoppable, and then I've

added the Redis option to

socket I/O because in a single server setup, socket

I/O doesn't really need to worry about sessions.

It keeps session state of your connections in memory.

Like many different web apps and other apps with session,

you need to deal with them in a database if you're going to

run multiple copies of that same app, right.

In this case, we're definitely to be running two or more

containers of this in the cluster for redundancy, so we

need that sort of setup.

As you'll see later, I add a simple little true/false

on whether the database is connected.

That will allow me to add that to part of my healthcheck

for a simple setup of, is my app talking

to the database so that it can be ready for clients to talk

to it.

Down here under my database connection, under this async

retry, I will add the DB connected true once

the client has connected to the database.

So, this will just mark it as true. It's a pretty simple

little example, but it gets the job done here.

It makes sure that essentially, my connections don't start

going to this app before it's ready.

If we scroll down a little farther, you'll see that we're

looking at healthchecks. This is me adding URLs, or routes,

to this Express app for the different types of

healthchecks. I talked earlier about the difference between

Docker and Kubernetes healthchecks.

Docker has a single one that you can use either in a

Dockerfile, which is the case here.

I've updated the Dockerfile with a healthcheck, or in the

stack or Compose files.

This one, I've just set the single URL that has to do

everything. In other words, it has to make sure that the

app is healthy, as well as make sure that

it's up and started, and all that stuff.

I give some information here in the comments that you can

read on your own around what different things this does

or what it should do.

If you're running this in production and simply scrolling

down, it just makes sure that if I'm

DB connected, then return a positive, or

a 200 result, on that URL, or an error

state if something else is wrong and a database isn't

connected. If you keep going, I've got one for readiness

that's very similar to that, and another one for

liveness. Those are the two types of connections that

you can use for Kubernetes because since Kubernetes has

two, the liveness really is about, is this

container working?

And readiness is, is it ready for connections?

When you start up a Kubernetes container, you want to make

sure that the database is connected and it's ready

to listen for incoming ingress connections before

the pod starts to return a positive

result to readiness.

Then past the healthchecks, you'll see that I've added the

SIGINT and SIGTERM from a previous lecture where we talked

about signals, making sure that Docker takes those signals

and does certain things.

You can see that for both of those, I am simply doing a

shutdown function, and that function is

below where we do a server stop.

In this case,

if you saw at the very top, I'm using Stoppable, which will

track the connections and then properly give those sections

a FIN packet.

It doesn't work with websocket so it's a little iffy in

this particular app because it is using long polling

and websocket type connections.

So, it doesn't work in all the cases of this app, but it

will help with anyone who's not websocket based to properly

give them a FIN packet to tell them to reset their

connection, or re-establish it to the load balancer, giving

them the opportunity to go to a new server without being

hard disconnected.

Then we set our little flag for DB connected to false so

that our readiness check, and our healthchecks, will stop

turning true.

Then, it will do a pool in to stop the database

connection and then properly exit the app.

And that's the end of the file. Again, not really a full

production setup, but it does give you the requirements,

making sure you have healthchecks and you're ready for

proper startup. Making sure that you have

the connection tracking so you know how to shut those down.

Then, and most importantly, for proper shut down, you're

dealing with connections appropriately so that when you

recreate this new container on, presumably

an updated image as you're constantly deploying new

versions of your app, that this app will have a much better

rolling update or blue-green update deployment.

