You've heard me talk about proxies several times before in

this course. But right now, I want to focus on production

architecture for your proxies if you need them.

Specifically here, I'm talking about web proxies, or

reverse proxies, for the HTTP protocol.

The common ports there are port 80 and 443.

As you might have imagined, or as you've probably had

troubles with on your own machine, how do you get multiple

containers to all list on 443 and 80?

How do you get all these different containers to respond on

that port? The answer is a proxy that looks

at the URL, or the DNS name, or both.

And then routes the request to the backend containers based

on those incoming requests.

In this scenario, Nginx and HAProxy are the

two traditional options, and you can use those natively

in sort of their pure, official format.

But, you can also just search for Docker, or Kubernetes, or

whatever you might be using, ECS, something like that.

You might find proxy containers with those tools that have

specific scripts, or other additional tools in them,

to make it easier depending on your setup.

You've seen in this course I've talked about Traefik.

Traefik is sort of the new kid on the block.

I really like it just because it has features

that are specific to the problems that we're trying to face

with all these different containers running on either a

single host, or in a cluster, like with Swarm, or

Kubernetes, or ECS.

Traefik automatically does things like Let's Encrypt, SSL

certificates. It allows you to easily configure from the

command line without being forced to use a config file,

and lots of other benefits. They're getting ready to

release a new version at this point.

The 2.0 is still not quite officially released.

But, the 2.0 version of Traefik is going to have a lot more

functionality, including non HTTP protocols.

You definitely want to check that out even for your

production workloads.

I can't stress enough that early on in your design

of whether you're building new Node.js apps, or converting

from Node.js apps that are precontainer and

putting them into containers, that you consider

whether or not you need proxies at all.

If you are going to use them, how they're going to be set

up.

Here's a couple of network diagrams and different

ways you can go about considering your proxies when you

deal with Node.js containers.

If you were making Node.js apps before Docker,

you might have a cloud server over here, and your

users are over here.

You would probably just be running Node straight on the VM.

Then, a proxy app on

that same machine. So, it might be HAProxy, Nginx,

whatever.

Then, users are talking directly to that server,

and the proxy is listening on port 80.

Then it's probably passing traffic over to Node using

localhost.

All right. That's a pretty traditional model for a single

server, simplistic setup.

As we change to Docker, that changes

a little bit. If we just look at a single Docker

server, same scenario, server and user,

then we've got two containers.

These aren't bundled in the same container, right.

So, we have Node running in here in one container.

We've got your proxy app running in a different

container.

That proxy is published on port 80

and probably 443.

Then, it's using Docker networking on the backend to

pass the traffic over to the Node app.

It's not using localhost because that, in the proxy

container, would be the proxy itself.

So, obviously it's going to be using Docker networking

there. Then, the user would talk to the published

port of the proxy there. So, not a whole lot of difference.

But when you shift to multiple servers, that's

when you start to consider clustering.

When you need more than one server, and you don't want to

have to individually control each one.

I would note that in this situation, if you simply wanted a

couple of servers, and they were going to run your Node app

for redundancy on across a couple servers, you would just

do this same thing on a couple of machines, and you would

control them individually.

But, that starts to get tedious.

Let's look at a different example.

In this case, we've got three servers running Docker,

and the user needs to get that same website.

So, multiple, different options exist here.

You might have a container with Node running in it

on each box.

You might have a proxy on each

one of those as well. We've got Node and proxy, Node

and proxy, and that same setup, right.

So, that's a simple design.

It's probably not good for orchestration because typically,

in an orchestrator like Kubernetes, ECS and Swarm,

you're not going to have the exact, same number of Node

processes as you would have proxies.

You probably just want enough proxies to be redundant,

maybe two or three.

But, you might need eight or ten of

your containers running your Node app, just for either

performance or capacity reasons.

Depending on which orchestrator you use, you might have

one or two servers using DNS round-robin,

where the user's browser would be getting different

connections to different servers.

Then, they would be connecting to the proxy there, just

like we saw before.

Then, the proxy on the backend would go to the Node there.

But, when you're in a cluster like this, you really want to

use all the Node apps.

So, depending on your orchestrator, you're going to be

using this backend network in the orchestration

so that your proxies pass the connections into

that network.

Then, that network maybe has a load balancer, or

something that's going to sit in front of your Node apps.

So, it's another layer.

Then, your traffic would pass to your Node apps on each

individual box. Go back out, depending on the

routing rules in round-robin and all that sort of thing.

Again, how you set up that proxy is very specific

to the orchestrator and tooling that you're using.

So, you're definitely going to want to spend some time

understanding whether there's a built-in proxy for that

system, like Ingress for Kubernetes.

Or, whether you're going to need to add something on your

own, like with Swarm.

Maybe you might have this other option where if

you're using something like ECS, or Kubernetes with its

Ingress with an external load balancer, you

might have something up front.

This is a different design where the user's hitting

a load balancer that's external,

Like an Amazon ALB or ELB,

that sits in front of your servers.

And, in this case, you may be able to get away with not

running the proxy at all, depending on how you argue

using your proxy. If you're simply using your proxy for

port traffic, and you only really care about

making sure that all your Node apps are responded to, your

load balance can pass traffic over here.

Maybe this one's running on 8080.

Maybe this one over here is running on 4320.

It can pass the traffic to these in

a different fashion.

I've just picked random ports here because depending on

your design, these containers may spin up on different

ports. It's the job of your orchestrator, back

here, to let the load balancer know

where all the Node containers are running and how to get

to them. Of course, with the different designs I've shown

here, there are lots of middle ground.

There are options where you use an external load balancer

and proxies inside your containers.

Really, if you can make it up, it's probably been done

before. This is important to understand.

How are your connections coming in?

Then also, how are your services

inside your server, or cluster, reaching each other when

they need to?

