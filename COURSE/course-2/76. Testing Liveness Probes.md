Now that we've updated the rng app, let's

break it. Let's send a bunch of load to the rng

while we have the liveness probe running.

We'll be watching the system to see how it reacts

to an excess amount of traffic that

will essentially cause the liveness probes to fail.

So, let's set the stage for how this is going to go.

We need to monitor some things while the liveness probe

is happening while also sending a bunch of traffic.

So first, we need to get the clusterIP of the rng

service. First, you're going to do a kubectl get service

rng.

Then, we're going to do some watch command

lines. We're going to watch the pods.

And a new one, get events.

Events, think of events as

a log of cluster events.

Things that are happening in your cluster, creating new

pods, stuff like that.

If it's got some stuff in it after you've

hit this command, don't worry.

Ignore all the previous events.

Basically, the events stores an etcd

and it only keeps for, I think, a few hours.

So, it's not going to keep a forever log in the etcd

database of system events.

It's really meant for short term event tracking

like we're doing. So, you're going to type these two

commands as well.

Then, we're going to jump in our shpod so

that we have access to the clusterIP.

We also are going to use a new utility called Apache

Bench, Apache Bench, similar to httping,

Apache Bench is designed to test a HTTP

endpoint, but its main purpose is

to throw a ton of connections at something all at

once. So, with this command, we're going to have

ab, which is the Apache Bench binary.

We're saying -c 10, which means

concurrent connections.

So, it's going to send HTTP connections

to rng, at a specific URL.

Then, we're going to tell it a total of 1000 connections,

but 10 at a time.

So, we're controlling the rate, as well as the number

of requests. Then we're giving it the endpoint

of the ClusterIP for the rng itself.

If you add the /1 on there, that will ensure

that we're using the actual rng number

generator to get this done.

If we did it on just the root of the rng,

nothing would happen. That would be a much simpler

healthcheck because it's simply testing whether or not the

container is responding on that port.

But, if we put the /1 in there, we're generating load

by causing the random number to be created.

So, I'm going to get here on the left, my

ClusterIP. Now your's, of course, will be different.

But, I want to use that inside the shpod.

So, we're going to do the same attach to shpod.

If you download Apache Bench yourself on your

machine, and you're also able to talk to the ClusterIP

directly because maybe you're on Linux, then you don't have

to do this shpod. Same rules apply as always.

I'm going to jump in that pod, and this is where

I will be able to type the Apache Bench command.

Over on the right, before we run that command, I'm

going to watch events.

And again, you can ignore any previous events.

They're just from previous lectures or things that you're

doing. Then, we're also going to watch pods down

at the bottom. The important thing is to watch the

rng and see the restarts column.

That restarts column will increment

as we have failed pods because of a failed

liveness check.

Over on the right, I'm going to type ab -c

10 -n 1000

http and then the IP address of

the rng, and then a /1 on the end to get that correct

URL, right.

So, we've set the stage.

Let's go back to the slides real quick because I want to

point out what we think is going to happen, because

I know what's in the code.

One thing I glossed over a few minutes ago was

exactly what the rng is doing.

In a real world app, there's going to be real load

whenever there's requests, right.

But in our case, this is a simple test app.

So, I didn't want to consume all the CPU resources

on your machine. So, instead of doing a bunch of intense

load, we're just adding in a sleep.

So, if you go look in the code of these apps, you will find

sleep statements in there.

That is a nice way for us to simulate

how long it takes for something to happen, right.

Your applications and the stuff that you're going to really

deploy on your clusters, they will consume real resources,

like memory, and CPU, and I/O, and networking.

But, I didn't want to do that on your machine, especially

if you're like me and you're on a laptop.

So, that's why these sleeps are in the code.

But, they do simulate a real response time,

which is it will take a hundred milliseconds,

approximately, for the rng to get back

to you when you request a random number.

If we give it 10 of those, because

it's single threaded, that will take around a second

for it to do.

If you think about our liveness probe, our liveness probe

is using the default of a one second response time.

So, if the application, in our case, is single

threaded and we're throwing it 10 requests at a time,

and we're expecting the liveness check to also respond

in the second, we might have a problem here.

Let's go run it and find out.

All right. I've got everything watched on the right and on

the left, I'm going to hit enter on the ab.

You'll see the Benchmark starts up and

it'll start telling us, down here on the bottom left,

what's going on as it starts to generate the load.

We can already see, on the right, that we're starting to

see liveness probe failures.

And remember, it takes three of those failed in a row,

based on our defaults, for it to kill a container.

So, now that's already happened, and it's going to kill

the container and restart it.

You can see over on the left, we've got a certain number of

requests completed already.

We're on our way up to the 1000 that we had asked it to do.

Remember that our app isn't engineered

correctly, so it's not a proper shutdown, which means that

the container's simulating a hung container at this

point. It's not responding, and it's not shutting

down healthy. So then eventually, we have

to kill the container. You'll see that stuff up here.

Then, it's going to check the pod

and restart it.

You see that and now you'll see down at the bottom here,

right next to my head, that we have

one restart now on rng.

Now, the interesting point here on the left, you'll notice

that we stopped...it actually stopped

responding.

Just Apache Bench gave up.

The reason they gave up is because when there was no more

containers, because our container is one right now,

we have one pod.

When that became unresponsive, and it had to be

killed and restarted, that essentially meant that

there was no pods responding to the load balancer.

That ClusterIP is trying to act like a load balancer, but

it can't send connections if there's no pods available.

So, it was rejecting requests.

It was resetting them.

That's what Apache Bench started to complain about and

said, well, if it's not even accepting my connections, I'm

just going to give up because there's no point in retrying.

So, it retries a couple of times and then it says, I give

up at 429 requests.

Now, if we scaled up our deployment and we

had multiple pods like you would in a real production

scenario, you would have a number of pods here for fault

tolerance and load.

Then as a pod started to terminate,

the load balancer would stop sending it new connections,

and it would depend more on the other pods, temporarily,

until it gets the broken one restarted.