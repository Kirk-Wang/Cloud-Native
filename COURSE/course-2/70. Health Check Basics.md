Healthchecks are

one of my favorite topics. Because I've spent so many years

as an operator that the idea around an

orchestration system that's managing my applications and

their lifecycle, could do basic checking

of my app and then do something about it if

it doesn't do what I'm hoping it would do as an

application? That's really cool.

That's a benefit of the orchestrator in addition to what it

already does. Let's talk about it.

First up, healthchecks are per container.

So, this is something that, if you think about it, a...even

if you had two containers in

a pod, the different containers would have different

things that you need to check, right.

So, one might be a web server.

One might be the PHP Backend, and they would have different

stuff. So, you're going to do this per container.

And across all the features that we're going to talk about

in healthchecks, what's going

to happen is that Kubernetes will run the healthcheck, and

if it fails, it will do something about that, depending on

all the settings. We have three types of probes

that would happen for different reasons.

The one that we all want to have in every container is

liveness. That simply is this

application working? I mean, it's a pretty pretty basic

question, but we'll dive into those details.

The next one is readiness.

That's only necessary on services, typically,

where you want to check, is

this ready for connections from the outside? Is it ready to

receive work? A new one that's in alpha state

in 1.16. Alpha, meaning it's brand

new and it's not even a beta feature yet, is the

startup. That's about waiting for a

container to finish its startup before it starts

the other two healthchecks. For each one of these,

we have three types of handlers.

That's HTTP, which is for websites

and Web API's, and stuff like that.

It'll read into the HTTP response and

see what's coming back. We have TCP, which is a

generic TCP connection just to see whether or not the

port is open. Then we have the exec.

That's sort of a fallback option that runs any

processor script inside the container that you want it to.

Now, all of these things are handled by the kubelet

agent on each node against the

containers on that node. The key thing to remember here is

that this is not a full monitoring

solution.

This is not meant to replace a third monitoring tool or a

remote monitoring system.

This is really about just making sure that container is

doing what it's supposed to be doing at a very low level,

and then doing something about it if it's not.

The first up, this is the one you need to focus on the

most. It's the liveness.

That's something that, again, every one of your containers

should have in it. It'll take you some effort on some of

your more complex applications, but it's totally worth it.

This is designed to basically check, is

this container alive?

Is it alive? Great.

Keep it running. If it's dead, we're going to kill it

and restart it. And

I say restarting, but in some cases, which aren't as

common, you might not want it to restart.

You may want it to just stop, and sit

there, and do nothing.

But, it's far more common to have the on failure and always

set up so that your application comes back after

a restart

in case the healthcheck fails.

The way to know whether or not you need

a liveness, versus one of the other types of probes,

is would this be something where if a human

was checking this, they would just restart the app?

If that's true, then it's a liveness situation, and

you want to implement that in liveness.

Sometimes, it can be a little challenging to figure out if

you need liveness or readiness.

This is sort of the approach for liveness is, would I

restart it? If yes, it's a liveness check.

Readiness probes are focused around multi-container

services.

Really, it's about whether or not we should

be sending connections to that container right now.

So, readiness is something where the container could go out

of readiness state. Maybe it's too busy.

Maybe it's overloaded and then it can come back, right.

This would be great for, you know, APIs

or Web servers where you get a boost of traffic, and

you get really slow in the container, and you

maybe you want to back off that by just taking it out of

the pool of available replicas in

the service. That's what readiness does is it removes it

from the service pool.

Really, there's two, main reasons when I think about

readiness. It's, is this thing overworked

right now, and we just need to give it some time to process

before sending it more connections and to make connections

work?

Or maybe, there is a another service

that this container depends on?

When I say service there, I just mean anything else it

might depend on. It might be Twitter. It might be some

external API. It might be something else that is necessary

for this container to properly serve requests.

Maybe that thing is down.

So, we're going to remove this from the pool

and not make it accessible either, since it won't function

without that backend system.

Startup probe is new in 1.16.

So, you won't see that in older versions, in case you still

have to run an older cluster.

But, you could technically get away with it before.

We had to add something else to

the other checks called the InitialDelaySeconds.

The problem with that was that it

would always require your container to wait that

amount of time before it could do readiness or liveness.

But, it tended to be necessary if you had slow

startup containers. That's sometimes normal.

Sometimes you have apps that need to build a cache,

or they need to do a database migration of something, just

depends on the system. So, you might have a long startup

period, and that might

not always be consistent. So, there wasn't a lot of ways to

do it in a flexible manner.

Now, we have this new option that we can have a specific

check, and once it starts to pass that

check, it will then enable the other probes.

So, the liveness and the readiness won't

even start until the startup probe is good.