All right. Our goal here is to remove one of the pods from

the load balancer.

That service is still pointing to both the DaemonSet

and the deployment. We need to talk a little bit more

about why this is happening and

how we go about changing it without tearing

the whole thing down and breaking everything.

Imagine you're in production maybe, and this is

something you have to fix without disrupting service.

Ideally, we can remove a pod by doing

something to it, and then that service will go back to

the proper setup of only pointing at one resource type.

We could just use a delete pod command.

That's sort of a brute force method, but we could try

that. The problem is, is that these resources are smart.

Both the DaemonSet and the deployment are expecting, in my

case, one container sitting there

in a pod. If it doesn't see that pod and its

selector, it's going to recreate another pod.

What about editing one of these pods and removing that

label, or just using a label command to take away the label

from a pod?

Well, if we did that, then the same thing would happen.

The resources that DaemonSet and the deployment

would see a pod missing, whichever one we edited,

and it would create another pod.

So, neither one of these scenarios is going to help us.

It's sort of perpetuating the problem.

This takes us back to the origin of what is the

responsibility of these particular resources.

Well, the particular resource of a ReplicaSet, which is

working for that deployment.

That ReplicaSet's goal is to say there needs to be this

many replicas matching this pod

spec, meaning that selector.

Then the DaemonSet's job is to make sure that there is one

pod per node.

It doesn't care about replicas.

It's not looking for that. It's just making sure that every

node as one.

This brings up a really important point that I want you to

remember is that when you create these resources,

it's not guaranteeing that the template for

the spec of that pod will always stay the same.

These things aren't doing it that way.

They're only looking at the

selector to make sure they're matching and it meets

their requirements, whether it's a ReplicaSet or whether

there's one per node.

It's simply doing that label selector.

It's not going deeper and then validating

that template. This is not a built-in feature of these

resources. This allows us to do some

potentially bad things, as an administrator.

We could go in and muck with these pods,

changing them in ways, and the particular

resource that's managing it may not know that.

We could go in and add more labels.

That wouldn't really change the way that the container was

behaving. But, just because we added more labels, it

doesn't invalidate that selector.

That means that the resource isn't going to recreate the

pod. I hope you understand that it's simply this little

selector. That's all that's really connecting them.

It's not a validation engine for ensuring

everything is identical is the way you wanted it.

Then, this creates another question if you start to think

about it. If they're all just looking for app

rng, if all these things, the DaemonSet, the ReplicaSet,

the deployment, the service, if they're all just looking

for app rng, why aren't they seeing each other's pods?

Like, they're they're not mixing and matching at the top.

It's really just the service that's the problem.

That has to do with a subtle, little difference in

what happens to them when they're created by our commands,

or our YAML.

If you go in and described the

ReplicaSet, which we really haven't dug into yet, but the

ReplicaSets, you know, managed by the deployment.

If we went in there and looked at that resource,

we would see that it's selector is more specific.

It's looking for two labels.

The app rng, as well as something called the pod

template hash, which is part of that

name of the pod that we see whenever we do get pods.

The same way is with the DaemonSet.

It also is looking for not just the app=rng.

It's looking for this controller revision hash, which is

essentially the same thing. It's going going to be a

different hash, though, than the one for the ReplicaSet,

and their key is different.

So, they're definitely not going to be able to overlap with

each other or interfere with each other.

So, that's good. At least this all seems to be just

related to the selectors for the service, and that we

didn't create a bunch of other problems in the process.

