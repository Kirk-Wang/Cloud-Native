We've talked about connections and replacing containers

before in this course.

Earlier in this course, you learned about SIGTERM and

various ways that Docker asks your container

to shut down.

I want to reiterate that now and go over some more details

about how that's going to be important in production.

First up, as a reminder, Docker will normally give you a

SIGTERM as a way to communicate, hey, you should start

shutting down, and that I'm going to give you a certain

amount of time to do that.

So, out-of-the-box, that's SIGTERM.

You can see an example of that in the sample directory

I show here, and you used that before actually in an

assignment earlier in the course.

A reminder of what that code really does.

It helps Docker not kill your process, but

it doesn't mean that you're getting true connection

migrations from your current container to a new one when

we're talking about production. In production, we're

talking about rolling updates, or blue-green deployments,

where there's different styles.

Basically, that's what these terms are is different styles

of changing out one working container that's in

production, receiving work and doing work for

a new one that's presumably got some better code,

or you know, whatever reason you have for replacing a

container. And, that you want to make

sure that none of those connections are terminated in

a bad way. They're not killed.

They're not turned off suddenly.

They're given proper time to shut down and all that kind of

stuff. Of course, this change is depending

on the protocols you're dealing with.

But, a lot of this we have concern around is

through web connections, or client connections, incoming

to our container. Basically, customers that we can't

control, necessarily, that they are there all the time.

They always need to have access to this service.

So, we need to basically, somehow, tell those connections

that we're stopping, and they need to go find another

container to use.

Quite frankly, we could spend an hour, or more, talking

just about this process.

But, I want to give you some quick, actionable stuff

you can take and put into your Node.js apps to help them.

The first one up is Terminus.

It's a boilerplate package that you can add into your

apps from GoDaddy.

They created this open source little app here.

It basically gives you the default, out-of-the-box,

minimum stuff you need to make sure that when Docker asks

you to shut down, that your container does shut down

properly. Of course, you're going to have to put in your

own application logic for what your application, itself,

needs to do before shutting down.

Maybe it needs to tell connections to terminate.

Maybe it needs to close database connections.

Stuff like that.

But, it gives you that boilerplate of healthchecks and

shutdown. Those are the two things that every container

really needs to do is your Node apps, whether

it's web, or worker processes in the background,

or whatever, it needs to be able to shut down properly.

So, have that proper shut down logic for SIGTERM.

Then it needs to have some sort of way of communicating

healthcheck, either to Docker, or Swarm, or

Kubernetes, or whatever else you're using.

I've given you a bunch of resources

in this lecture to go read.

They're important because some of them are example repos on

how other people have solved the problem.

Some of them are great guides, from other blogs,

on how you manage shutdown and replacement of containers,

and stuff that is specific to Node.js.

So, I recommend that you read them.

Some of those are great resources that help me understand

the problem at a deeper level as well.

When we get into the nitty gritty of regardless

of what tool you're using, how you replace a container

that's running, it comes down to a few, key steps.

The first step is Docker will communicate to

your container that it needs to shut down and then it

waits. Now, Docker can't see really what's going on in your

container, nor can Kubernetes, or Swarm, or any other tool.

It's not going to understand one container's app from

another. But, it does wait for

you to properly exit a certain amount of time.

Out-of-the-box, Docker and Swarm default to 10 seconds and

Kubernetes defaults to 30 seconds.

That's the time it gives you to shut down all the

connections and processes ,and clean up whatever you need

to for proper container healthy shutdown.

You can extend both of those.

You can change both of those numbers on either side with

YAML, depending on what tool you're using.

It's pretty easy just to Google that and find the answer.

Orchestrators have subtle, but important, differences

in the order and way that they stop new connections

from coming to your container, how they move those

connections to a new container, and then not

only on how long they wait for your shutdown, but how they

let your container shut down.

One of those important distinctions is around load

balancing, also known as ingress when you're talking about

orchestration. Those connections coming in from your

clients, one of the first things that needs to happen

before your app and your container can stop, is that those

load balancers need to start moving new connections,

not existing ones, but new ones that are incoming,

to a new container so that you can start cleaning

up work and resolving all the existing connections, right.

You've got to finish the work you've started so that you

can shut down cleanly.

With Swarm and Kubernetes, they are different.

Swarm will actually talk directly to the load balancer for

you before it even starts the shutdown, and tell the load

balancer to start sending new connections elsewhere,

assuming that you have another container or replica

running.

Kubernetes will expect your container to have a readiness

check that starts to fail so that it knows to signal

the load balancer to send connections elsewhere.

These are more about the orchestrator, really nothing to do

with Node.js. So, you're going to want to invest some time

learning those tools and how they treat container

replacement.

My tip here is that you definitely want to give

your shutdown time, that's the time that

your orchestrator, or Docker, or whatever tool you're

using, waits for the container to finish shutting down.

You're going to want to give that timer enough time for

your long polling to stop.

So, if you're using HTTP, and you're familiar with long

polling, you might tell your clients to check back every so

often. Maybe every minute for data.

And that's depending on your app, of course, of how you've

built it. You're going to definitely...one of those tips

there is to make sure that that wait is longer

than that long polling, so that you can at least tell the

clients, hey, I'm terminating my connections.

I'm ending my connections. You need to go somewhere else.

In order to do this, there is a subtle, but

necessary, step.

That is with HTTP specifically, that

you need to start metering, or managing, connections

one by one. By default in Node, it doesn't

count connections. It doesn't track connections.

So, you can't control, or manage, each one individually.

Once you want to replace a container, you need to be

able to tell those connections a specific packet, a thin

packet, in HTTP that says, I'm done with connections.

Please re-establish a brand new connection, which the load

balancer will send somewhere else.

Your container needs to be able to tell them that.

As of right now, the only tool

that I know of that you can just plug in, as a module into

a Node app, is Stoppable.

So, I've given you resources here on Stoppable, which

I know others have used, and it seems like a pretty legit

piece of software. I have used it before.

What it will do is it tracks connections.

Then once you give it the signal to shut down in a

container, or in any Linux app, right.

It's a regular Linux shutdown.

It will start to tell existing connections,

hey, you need to restart your connection even though I know

you already have a connection with me.

Restart a new one, and that way the load bouncer would send

them somewhere else. That's a thin packet.

So, spend some time learning about that if you're not

familiar, and understanding how existing connections,

and new connections, work in an HTTP app.

Then you can put these modules into your Node apps

and hopefully solve this problem easier than just writing

it from scratch.

