Hopefully you have finished the assignment.

Maybe you've checked out the assignment answers, and then

you've tried the bonus material.

The reason I put the bonus in here was it's relying on

two things. One is you've got to go back to previous

lectures earlier in the course, and depending on how fast

you're consuming all this content, it may have been days

ago, could have been weeks ago, that you did that stuff.

It's always hard to remember, and that's why I'm bringing

it back. You're definitely going to need

to repeat this stuff over and over to remember it long

term. So, here's another chance.

I'm glad you're checking out the bonus and you're

considering doing that yourself. Hopefully, you've tried

all that. You've gotten it to work, or maybe you got stuck,

and you're here for that reason.

That's all fine because this is, one part, going

back to those other videos, and the other part is doing

Internet research. A lot of these things, if you did it in

a previous video, you maybe didn't do it exactly the same

way as you'll do it here.

Or, maybe it's something that we only mentioned and didn't

practice in a previous video.

That's all good stuff because in the real world,

you are going to be doing stuff every day that you probably

have never done before. That's the life of tech, right.

Especially in DevOps where everything is new.

So, that's a good pattern to practice.

Here we go. I'm in the bonus area and the first

thing it tells me is security scanner.

We had a whole lecture on that earlier.

I'm going to go ahead and do the same thing as we did

before, only we're going to fit it in this Dockerfile we

just built. You've got the Dockerfile that you built

in the previous assignment answers.

We're just going to keep rolling through that.

We're basically building on that answer file.

Down here in the test

area, is where I probably want to put the scanner.

I could make it it's own, separate

stage. That way I could do code testing and not

security scanning.

And, you know, leave them a little bit flexible such you

choose one or the other when you build the image.

But, I'm just going to add it in here because it doesn't

require, in the readme, that I do it in a separate stage.

And since I'm going to use the MicroScanner, let me just go

to the Web and go find that GitHub repo.

Like all good GitHub repos, it's got the basic

use and install stuff right here in the page,

just like we used before.

In fact, if we go back to that previous lecture in

the course repo, I can probably cut and paste from there.

That example was in the multistage scanning

directory. The Dockerfile in there was a multistage

Dockerfile that had the MicroScanner included.

I'm going to copy and paste this out.

We're going need to change something.

You'll see it in a second that,

what's wrong with this line.

This is APK. This is for Alpine.

But, what are we using in this example?

We're using the Node slim.

Node slim is Debian based.

If it was going to be Alpine based, it would say Alpine

somewhere in the name or tag there.

We definitely can't use this line.

When you go back and look at the repo though, the repo

says, add ca-certificates if needed if you're on an apt-get

based distribution.

That's what we're using.

We just need to get the ca-certificates.

You might have remembered seeing that.

If we go back into the repo here and

scroll up in our Dockerfile, you'll notice that we

installed the ca-certificates already in dev.

That was for the dev dependencies.

It just so happens we'll get to use that again here, so we

don't even need this line.

We can take out the APK, and we don't need to add

back in an apt-get line because ca-certificates is already

there. We're going to use an argument in

this case, which means we need to specify at the command

line. What I've done at the command line, just like in the

previous example, is I've created my own environment

variable, in my session, that stores

that key.

Then I don't need to specify that key every time.

I'll save this and we'll go run it and see if we can get

the test to build.

We're going to need to do a manual build here.

So a docker build.

I'm going to need to add in that build argument for the

MicroScanner. So, inside the Dockerfile is called

microscanner token.

On my host, I've set it as an environment variable,

just called microscanner. Your's might be different.

Then I need to tag it, so we'll just call it ultimate

node test. Then

I need to target it to the stage

test. Then I am building in this directory.

All right. My scanner ran, and worked,

and that's good.

Moving on. Back in the readme, the third line here is about

the best practices from that earlier section.

So, let's enable BuildKit and try again.

In order to use BuildKit, I could do something like

export the docker buildkit equals

1, but I could also just rerun that last command.

Then add in the beginning of it docker

buildkit equals 1.

Then, yep. You'll see that it is now using the new

interface to build that image.

I'll assume that's going to finish properly.

So, I'm going to go back over and check the next line.

Add Tini to images so containers will receive shutdown

signals. We could go look that up,

right. We could just do docker runs or docker compose and

manually add Tini in. But in this case, where you have

an app that doesn't have signals in it, it's not accepting

signals built into the code, Docker recommends that you

install Tini into the image as a permanent part of your

app, if you have no plans to update your app.

Because your users, people that might run this

container, or run Docker Compose, may forget to throw in

Tini. So, you want to make sure it's always

there and always working.

Let's go back to the browser and go

look up Tini.

Down here, there

is some install info.

What I'd want to do here is copy

this out, whatever version is the latest Tini version.

Then add it in my Dockerfile.

If I add it as an entry point up here

at the very root, then

it should be used in all future

stages. Which means, that the entry point

here, which essentially puts Tini on the front

of any of your cmds, will always be in front of the cmds of

future images.

So, we only have to put it in once.

It will, though, cause us to do a complete rebuild because

it'll bust the cache and require all future stages

to be built again. If we do that one more time, just hit

the up arrow, do the BuildKit.

All right. So, that built on the test with that new Tini in

there. Let's build and run the production version

with Tini, just to make sure it works as expected.

I'm going to change this to prod.

I'll change the tag to prod.

All right. That should have built pretty quickly because

BuildKit is more intelligent at caching the layers.

Now I can do a docker run, and I shouldn't need to run

--init. I can just run this ultimate

node prod.

Of course, it'll be waiting for the DB because it doesn't

have a DB be running in the background.

But, I can control c.

And on Mac and Linux, it will actually stop the container.

Remember on Windows, it doesn't actually stop it.

The way you could test this on Windows,

if you remember from the previous lecture, was you do a

docker stop, and the name, or ID, of the container,

and if it takes more than a few seconds, then you know you

have a problem with signals.

All right. Enable the non root Node user for all dev

prod images.

The next one after that is you might need root user for

testing or scanning images, depending on what you're doing.

This is an interesting conundrum.

In this case, we're going to be in the Dockerfile.

Let's say that up here at the top, I

would want to add the user Node.

Well, the problem with this is that any future

stuff, like this apt-get down here, wouldn't work because

it requires root.

So, maybe I shouldn't be putting user Node up at the top.

I should put it down closer to the bottom.

Ideally in development, I'm also running as user

Node. That way I find permission problems that are

potentially going to cause production problems.

So, let's go ahead and put that here

right before nodemon runs.

Unfortunately, that particular

line right there is not going to be passed into production.

So, we're going to need to repeat this line, but that's OK.

It's just a metadata line.

It's not a huge deal.

It's not going to slow down anything in the building.

It is not pure dry code, as

you would call it. But I think we should allow

it because it gets things working.

So, we're back down here user Node.

Then it will

change the user to that Node right before it starts.

So that means when the MicroScanner runs, it will run as

root. What's interesting, though, is we got another problem

here. At least I'm predicting a problem.

Is that the user Node up here in the dev is

going to be rolled into the test.

Which means, all these things will run as user

Node, including the MicroScanner.

The MicroScanner probably needs root.

I don't even know. I haven't checked, but most scanners

need authorization to see all the files in the file system,

and if they're not root, they usually can't do that.

Let's go see if this actually works.

I'm going to do my build again and do the production build.

Which in the case of our Dockerfile, we'll end up building

all stages, because it needs the test output

in order to get the files.

So, let's do that and see what happens.

There we go. All right. We're getting a chmod issue

because run can't change the mode, because

that normally requires root.

So, we are going to go back and forth a little bit.

We've got the user Node for the nodemon command here.

Down here, we're going to need to change user again to user

root.

Then that should mean this line will work properly.

That's okay that it's there.

It doesn't need to be set back to Node because we've

already got the Node down here in production.

All right. I'm just going to hit the up arrow and try again

with the prod image.

That time, the change mode worked.

And there we go. A successfully built file.

I'm going to hit the up arrow a couple of times back to

that docker run.

I'm going to run the production image just to make sure

that it does run as expected with the new user.

Here, you will see an interesting,

new area that we haven't talked about.

I kind of threw this in by accident

when I ran through the material initially.

But, then I realized this is a great example.

In Linux, you may not know that you can't run

non root applications on low ports.

So, by default you need to be root to have the permissions

to use port 80.

A port like 4000, or 3000, works

fine as a regular user.

But, those low ports are reserved for root only.

If you did some Googles, you would find out pretty quickly

that this needs root to run.

That doesn't mean we should be running our app as root in

the container. That doesn't mean we should back up and just

punt and say, I have to use the root

user. That's not true.

In the container, it doesn't really matter, in almost

all cases, what port we use.

Because if we're going to use it outside of that

Docker network, we're just going to publish it on whatever

port we want. So, it's going to do that translation.

Inside the container network, we are probably able to tell

other apps what port this is on.

So, we just need to go over and change the default port

it's using.

If we scroll up, of course, we have the expose here.

I'm just going to use 8080 in this case.

But, that doesn't change what the app listens on.

In this case, because it's a Node app, hopefully you can

dig into the Server.js and find that

the port, right here, is defined.

If you're not receiving a port as the configuration,

then it defaults to 80.

What we want it to do is run on 8080.

It'll default to that one.

Between that and the Dockerfile, let's try

to rebuild and run again.

Then I'll try to run the same image.

There we go. App is now running on port 8080.

It's waiting for DB.

If control c, or on Windows docker

stop. There we go.

Both of those things are working together.

That kind of wraps up the bonus here.

I hope you got through that. If you have time for the bonus

because I think a couple of those scenarios, down at the

end there, are pretty common dealing with going back and

forth between the Node user and the root user.

Dealing with low ports and Linux because a lot of people

may not realize that. That once they locked down the user

to non root, that they're restricted in that low port area.

They can't necessarily use the low ports.

That stuff is common in dealing with

Docker containers day to day.

